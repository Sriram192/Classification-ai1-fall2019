{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.6.8",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "lab_clfmetrics.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkL8OXeOB9E1",
        "colab_type": "text"
      },
      "source": [
        "# Classification, Probabilities, and the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeR5MHchB9E7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4ff8ebb2-2bf7-4b9c-b0b9-2667530d5404"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn.apionly as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:855: MatplotlibDeprecationWarning: \n",
            "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
            "  \"found relative to the 'datapath' directory.\".format(key))\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:846: MatplotlibDeprecationWarning: \n",
            "The text.latex.unicode rcparam was deprecated in Matplotlib 2.2 and will be removed in 3.1.\n",
            "  \"2.2\", name=key, obj_type=\"rcparam\", addendum=addendum)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/apionly.py:9: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.\n",
            "  warnings.warn(msg, UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-d_tCQEB9FA",
        "colab_type": "text"
      },
      "source": [
        "We are going to encapsulate some code into handy-dandy functions that we can use for easier model training using cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coKshKt5B9FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
        "    if score_func:\n",
        "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
        "    else:\n",
        "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
        "    gs.fit(X, y)\n",
        "    print(\"BEST\", gs.best_params_, gs.best_score_)\n",
        "    best = gs.best_estimator_\n",
        "    return best\n",
        "def do_classify(clf, parameters, indf, featurenames, targetname, target1val,mode=\"mask\", reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n",
        "    \"\"\"\n",
        "    Classification made simple (or is it more complex?)\n",
        "    THIS WORKS FOR 2 Class Classification problems only\n",
        "    parameters: parameter grid in the sklearn style\n",
        "    indf: dataframe you feed in\n",
        "    featurenames: list of columnames corresponding to features you want in your model\n",
        "    targetname: the column you want to use as target\n",
        "    target1val: the value of the \"targetname\" column\n",
        "    mode: mask or split. mask a boolean mask to choose train/test or\n",
        "        split a dictionary with keys Xtrain/Xtest/ytrain/ytest and values existing\n",
        "        training and test sets in the canonical form\n",
        "    reuse_split: the actual mask above or the actuall ditionary, depending upon which\n",
        "        modu you chose\n",
        "    score_func: this is from GridSearchCV\n",
        "    n_folds: cross val folds\n",
        "    n_jobs: mumber of processes to use in cross-validation\n",
        "    \n",
        "    We return classifier, and the train and test sets. We print accuracies\n",
        "    and the confusion matrix\n",
        "    \"\"\"\n",
        "    subdf=indf[featurenames]\n",
        "    X=subdf.values\n",
        "    y=(indf[targetname].values==target1val)*1\n",
        "    if mode==\"mask\":\n",
        "        print(\"using mask\")\n",
        "        mask=reuse_split\n",
        "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
        "    else:\n",
        "        print(\"using reuse split\")\n",
        "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
        "    if parameters:\n",
        "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
        "    clf=clf.fit(Xtrain, ytrain)\n",
        "    training_accuracy = clf.score(Xtrain, ytrain)\n",
        "    test_accuracy = clf.score(Xtest, ytest)\n",
        "    print(\"############# based on standard predict ################\")\n",
        "    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
        "    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
        "    print(confusion_matrix(ytest, clf.predict(Xtest)))\n",
        "    print(\"########################################################\")\n",
        "    return clf, Xtrain, ytrain, Xtest, ytest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EueONViB9FE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
        "cm = plt.cm.RdBu\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "\n",
        "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.3, psize=10, zfunc=False):\n",
        "    h = .02\n",
        "    X=np.concatenate((Xtr, Xte))\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    #plt.figure(figsize=(10,6))\n",
        "    if mesh:\n",
        "        if zfunc:\n",
        "            p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
        "            p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "            Z=zfunc(p0, p1)\n",
        "        else:\n",
        "            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light, alpha=alpha, axes=ax)\n",
        "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n",
        "    # and testing points\n",
        "    yact=clf.predict(Xte)\n",
        "    ax.scatter(Xte[:, 0], Xte[:, 1], c=yte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    return ax,xx,yy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qim5loWzB9FH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1, prob=True):\n",
        "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha) \n",
        "    if prob:\n",
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "    else:\n",
        "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n",
        "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n",
        "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14)\n",
        "    return ax "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wv8ZeOjB9FJ",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the data\n",
        "\n",
        "(I encountered this dataset in Conway, Drew, and John White. Machine learning for hackers. \" O'Reilly Media, Inc.\", 2012.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUR0peZsDaB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "665f7efe-deb4-4b8c-d345-1e29232ce15c"
      },
      "source": [
        "!git clone https://github.com/univ-ai/Classification-ai1-fall2019\n",
        "%cd Classification-ai1-fall2019\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Classification-ai1-fall2019' already exists and is not an empty directory.\n",
            "/content/Classification-ai1-fall2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6faMTi5bB9FK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "00e04964-78d1-481d-b884-f74ba3aaac72"
      },
      "source": [
        "dfhw=pd.read_csv(\"data/01_heights_weights_genders.csv\")\n",
        "dfhw.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>73.847017</td>\n",
              "      <td>241.893563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>68.781904</td>\n",
              "      <td>162.310473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>74.110105</td>\n",
              "      <td>212.740856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>71.730978</td>\n",
              "      <td>220.042470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Male</td>\n",
              "      <td>69.881796</td>\n",
              "      <td>206.349801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Gender     Height      Weight\n",
              "0   Male  73.847017  241.893563\n",
              "1   Male  68.781904  162.310473\n",
              "2   Male  74.110105  212.740856\n",
              "3   Male  71.730978  220.042470\n",
              "4   Male  69.881796  206.349801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRxOHuzlB9FM",
        "colab_type": "text"
      },
      "source": [
        "We sample 500 points from 10,000, since we actually want to see trends clearly on the plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dryzqZmB9FM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60ea6ce2-a9e3-4b9f-dce7-c42c4dbeed35"
      },
      "source": [
        "df=dfhw.sample(500, replace=False)\n",
        "np.sum(df.Gender==\"Male\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "248"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEE5F5_TB9FO",
        "colab_type": "text"
      },
      "source": [
        "We split the data into training and test sets...and setup a mask so we can reuse these splits later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ7BeG-8B9FP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22a3eca5-53c3-42d0-af9e-484b68c31bf7"
      },
      "source": [
        "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\n",
        "mask=np.ones(df.shape[0], dtype='int')\n",
        "mask[itrain]=1\n",
        "mask[itest]=0\n",
        "mask = (mask==1)\n",
        "mask[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True, False,  True, False, False, False,  True,  True,\n",
              "       False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut2tedAXB9FS",
        "colab_type": "text"
      },
      "source": [
        "## Logistic regression\n",
        "\n",
        "\n",
        "$$\n",
        "\\renewcommand{\\like}{{\\cal L}}\n",
        "\\renewcommand{\\loglike}{{\\ell}}\n",
        "\\renewcommand{\\err}{{\\cal E}}\n",
        "\\renewcommand{\\dat}{{\\cal D}}\n",
        "\\renewcommand{\\hyp}{{\\cal H}}\n",
        "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n",
        "\\renewcommand{\\x}{{\\mathbf x}}\n",
        "\\renewcommand{\\v}[1]{{\\mathbf #1}}\n",
        "$$\n",
        "\n",
        "\n",
        "Previously, we saw the loss for Logistic regression and noted that it is a loss for probability estimation...and not a loss for making decisions. We'll go into these dual losses soon..\n",
        "\n",
        "$$R_{\\cal{D}}(h(x)) = -\\loglike = -log \\like = - log(P(y|\\v{x},\\v{w})).$$\n",
        "\n",
        "\n",
        "Thus\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "R_{\\cal{D}}(h(x)) &=& -log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n",
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n",
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n",
        "                  &=& - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n",
        "\\end{eqnarray*}\n",
        "\n",
        "where\n",
        "\n",
        "$$h(z) = \\frac{1}{1 + e^{-z}}.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ZU-ILyB9FT",
        "colab_type": "text"
      },
      "source": [
        "Notice that its L2 regularized.... by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keJu74pFEqnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.iloc[itrain][[\"Height\",\"Weight\"]].values\n",
        "x_test = df.iloc[itest][[\"Height\",\"Weight\"]].values\n",
        "y_train = 1*(df.iloc[itrain][\"Gender\"]=='Male')\n",
        "y_test = 1*(df.iloc[itrain][\"Gender\"]=='Male')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HICoxUGUEQep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "954b6ff4-600d-4db4-caab-bae525d7014e"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(C=1000)\n",
        "clf.fit(x_train,y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVB7R-akHDaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYZd1VSaHTem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "dae4e691-5188-434a-f5ba-aa4ad2875330"
      },
      "source": [
        "prediction - y_test"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-44ee2f7740fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m         return construct_result(left, result,\n\u001b[1;32m   1585\u001b[0m                                 index=left.index, name=res_name, dtype=None)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpressions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0muse_numexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mrsub\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,) (300,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OBds-qPB9FU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
        "clflog = LogisticRegression(solver='lbfgs')\n",
        "clflog, Xtrain, ytrain, Xtest, ytest=do_classify(clflog, parameters, df, ['Height','Weight'],'Gender', \"Male\", mode=\"mask\", reuse_split=mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HLEbmpzB9FX",
        "colab_type": "text"
      },
      "source": [
        "In `sklearn`, `clf.predict(test_data)` makes predictions on the assumption that a 0.5 probability threshold is the appropriate thing to do. Make predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cVAd0FfB9FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSDXtWE3B9Fb",
        "colab_type": "text"
      },
      "source": [
        "In `sklearn`, `predict_proba` gives us the probabilities. Find the probabilities on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm3znW3QB9Fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clflog.predict_proba(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9f42KA4B9Fe",
        "colab_type": "text"
      },
      "source": [
        "What do these probabilities correspond to? The second column (`[:,1]` in numpy parlance, google numpy indexing to understand the syntax) gives the probability that the sample is a 1 (or +ive), here Male.\n",
        "\n",
        "Make a histogram of these probabilities. Interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD1I9kaVB9Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0gaR2IpB9Fh",
        "colab_type": "text"
      },
      "source": [
        "Lots of sure females and sure males when you plot the probability of being a male. \n",
        "\n",
        "At this point you might want to see how this histogram looks in the 2 dimensional space of our predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R__xEwe4B9Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtr=np.concatenate((Xtrain, Xtest))\n",
        "plt.figure()\n",
        "ax=plt.gca()\n",
        "with sns.plotting_context('poster'):\n",
        "    points_plot(ax, Xtrain, Xtest, ytrain, ytest, clflog);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTcF0lSwB9Fk",
        "colab_type": "text"
      },
      "source": [
        "We can plot the probability contours: these are rather tight!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAhOR-EGB9Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "ax=plt.gca()\n",
        "points_plot(ax, Xtrain, Xtest, ytrain, ytest, clflog, mesh=False, alpha=0.001);\n",
        "points_plot_prob(ax, Xtrain, Xtest, ytrain, ytest, clflog);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jssC3ZjOB9Fn",
        "colab_type": "text"
      },
      "source": [
        "The score function of the estimator is used to evaluate a parameter setting. These are the sklearn.metrics.accuracy_score for classification and sklearn.metrics.r2_score for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). We can pass other scorers to `GridSearchCV`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDUoRpvhB9Fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clflog.score(Xtest, ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85cnuIqEB9Fp",
        "colab_type": "text"
      },
      "source": [
        "## The confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0sqWwawB9Fq",
        "colab_type": "text"
      },
      "source": [
        " A classifier will get some samples right, and some wrong. Generally we see which ones it gets right and which ones it gets wrong on the test set. There,\n",
        "\n",
        "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n",
        "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n",
        "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n",
        "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n",
        "\n",
        "A classifier produces a confusion matrix from these which lookslike this:\n",
        "\n",
        "![hwimages](https://github.com/Sriram192/Classification-ai1-fall2019/blob/master/images/confusionmatrix.png?raw=1)\n",
        "\n",
        "\n",
        "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the form above, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SHRpfC1B9Fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(ytest, clflog.predict(Xtest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16CkxqaEB9Ft",
        "colab_type": "text"
      },
      "source": [
        "Given these definitions, we typically calculate a few metrics for our classifier. First, the **True Positive Rate**:\n",
        "\n",
        "$$TPR = Recall = \\frac{TP}{OP} = \\frac{TP}{TP+FN},$$\n",
        "\n",
        "also called the Hit Rate: the fraction of observed positives (1s) the classifier gets right, or how many true positives were recalled. Maximizing the recall towards 1 means keeping down the false negative rate. In a classifier try to find cancer patients, this is the number we want to maximize.\n",
        "\n",
        "The **False Positive Rate** is defined as\n",
        "\n",
        "$$FPR = \\frac{FP}{ON} = \\frac{FP}{FP+TN},$$\n",
        "\n",
        "also called the False Alarm Rate, the fraction of observed negatives (0s) the classifier gets wrong. In general, you want this number to be low. Instead, you might want to maximize the\n",
        "**Precision**,which tells you how many of the predicted positive(1) hits were truly positive\n",
        "\n",
        "$$Precision = \\frac{TP}{PP} = \\frac{TP}{TP+FP}.$$\n",
        "\n",
        "Finally the **F1** score gives us the Harmonic Score of Precision and Recall. Many analysts will try and find a classifier that maximizes this score, since it tries to minimize both false positives and false negatives simultaneously, and is thus a bit more precise in what it is trying to do than the accuracy.\n",
        "\n",
        "$$F1 =  \\frac{2*Recall*Precision}{Recall + Precision}$$\n",
        "\n",
        "However, in a case like that of a cancer classifier, we will wish to minimize false nagatives at the expense of false positives: it is ok to send perfectly healthy patients for cancer folloup if that is the price we must pay for not missing any sick ones.\n",
        "\n",
        "`scikit-learn` helpfully gives us a classification report with all these numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X8uDQlRB9Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, clflog.predict(Xtest)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itImgy7zB9F5",
        "colab_type": "text"
      },
      "source": [
        "#### The cancer doctor\n",
        "\n",
        "Do you really want to be setting a threshold of 0.5 probability to predict if a patient has cancer or not? The false negative problem: ie the chance you predict someone dosent have cancer who has cancer is much higher for such a threshold. You could kill someone by telling them not to get a biopsy. Why not play it safe and assume a much lower threshold: for eg, if the probability of 1(cancer) is greater than 0.05, we'll call it a 1.\n",
        "\n",
        "Write a function `t_repredict(est,t, Xtest)` which takes your classifier, a probability threshold, and a  features set in the canonical form, and returns a set of predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JJI70mYB9F7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sURPjXcgB9F_",
        "colab_type": "text"
      },
      "source": [
        "Print the confusion matrix to see how the false negatives get suppressed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZGaN1SAB9F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}